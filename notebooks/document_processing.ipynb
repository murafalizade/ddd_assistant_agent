{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-19T19:25:31.761606Z",
     "start_time": "2026-01-19T19:25:25.135367Z"
    }
   },
   "source": "pip install pdfplumber pandas",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\r\n",
      "Collecting pdfplumber\r\n",
      "  Downloading pdfplumber-0.11.9-py3-none-any.whl.metadata (43 kB)\r\n",
      "Requirement already satisfied: pandas in /Users/muradeliyev/Library/Python/3.9/lib/python/site-packages (2.2.2)\r\n",
      "INFO: pip is looking at multiple versions of pdfplumber to determine which version is compatible with other requirements. This could take a while.\r\n",
      "  Downloading pdfplumber-0.11.8-py3-none-any.whl.metadata (43 kB)\r\n",
      "Collecting pdfminer.six==20251107 (from pdfplumber)\r\n",
      "  Downloading pdfminer_six-20251107-py3-none-any.whl.metadata (4.2 kB)\r\n",
      "Requirement already satisfied: Pillow>=9.1 in /Users/muradeliyev/Library/Python/3.9/lib/python/site-packages (from pdfplumber) (10.4.0)\r\n",
      "Collecting pypdfium2>=4.18.0 (from pdfplumber)\r\n",
      "  Downloading pypdfium2-5.3.0-py3-none-macosx_11_0_arm64.whl.metadata (67 kB)\r\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in /Users/muradeliyev/Library/Python/3.9/lib/python/site-packages (from pdfminer.six==20251107->pdfplumber) (3.3.2)\r\n",
      "Collecting cryptography>=36.0.0 (from pdfminer.six==20251107->pdfplumber)\r\n",
      "  Downloading cryptography-46.0.3-cp38-abi3-macosx_10_9_universal2.whl.metadata (5.7 kB)\r\n",
      "Requirement already satisfied: numpy>=1.22.4 in /Users/muradeliyev/Library/Python/3.9/lib/python/site-packages (from pandas) (1.26.4)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/muradeliyev/Library/Python/3.9/lib/python/site-packages (from pandas) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/muradeliyev/Library/Python/3.9/lib/python/site-packages (from pandas) (2024.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/muradeliyev/Library/Python/3.9/lib/python/site-packages (from pandas) (2024.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.15.0)\r\n",
      "Collecting cffi>=2.0.0 (from cryptography>=36.0.0->pdfminer.six==20251107->pdfplumber)\r\n",
      "  Downloading cffi-2.0.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (2.6 kB)\r\n",
      "Collecting typing-extensions>=4.13.2 (from cryptography>=36.0.0->pdfminer.six==20251107->pdfplumber)\r\n",
      "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\r\n",
      "Collecting pycparser (from cffi>=2.0.0->cryptography>=36.0.0->pdfminer.six==20251107->pdfplumber)\r\n",
      "  Downloading pycparser-2.23-py3-none-any.whl.metadata (993 bytes)\r\n",
      "Downloading pdfplumber-0.11.8-py3-none-any.whl (60 kB)\r\n",
      "Downloading pdfminer_six-20251107-py3-none-any.whl (5.6 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m5.6/5.6 MB\u001B[0m \u001B[31m9.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading pypdfium2-5.3.0-py3-none-macosx_11_0_arm64.whl (2.8 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.8/2.8 MB\u001B[0m \u001B[31m15.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading cryptography-46.0.3-cp38-abi3-macosx_10_9_universal2.whl (7.2 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m7.2/7.2 MB\u001B[0m \u001B[31m14.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading cffi-2.0.0-cp39-cp39-macosx_11_0_arm64.whl (180 kB)\r\n",
      "Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\r\n",
      "Downloading pycparser-2.23-py3-none-any.whl (118 kB)\r\n",
      "Installing collected packages: typing-extensions, pypdfium2, pycparser, cffi, cryptography, pdfminer.six, pdfplumber\r\n",
      "  Attempting uninstall: typing-extensions\r\n",
      "    Found existing installation: typing_extensions 4.11.0\r\n",
      "    Uninstalling typing_extensions-4.11.0:\r\n",
      "      Successfully uninstalled typing_extensions-4.11.0\r\n",
      "\u001B[33m  WARNING: The script pypdfium2 is installed in '/Users/muradeliyev/Library/Python/3.9/bin' which is not on PATH.\r\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\r\n",
      "  NOTE: The current PATH contains path(s) starting with `~`, which may not be expanded by all applications.\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\u001B[33m  WARNING: The script pdfplumber is installed in '/Users/muradeliyev/Library/Python/3.9/bin' which is not on PATH.\r\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\r\n",
      "  NOTE: The current PATH contains path(s) starting with `~`, which may not be expanded by all applications.\u001B[0m\u001B[33m\r\n",
      "\u001B[0mSuccessfully installed cffi-2.0.0 cryptography-46.0.3 pdfminer.six-20251107 pdfplumber-0.11.8 pycparser-2.23 pypdfium2-5.3.0 typing-extensions-4.15.0\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.3\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001B[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T19:34:46.663130Z",
     "start_time": "2026-01-19T19:34:36.679662Z"
    }
   },
   "cell_type": "code",
   "source": "pip install 'camelot-py[cv]'",
   "id": "934c0d9150f30ea5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\r\n",
      "Collecting camelot-py[cv]\r\n",
      "  Downloading camelot_py-1.0.9-py3-none-any.whl.metadata (9.8 kB)\r\n",
      "\u001B[33mWARNING: camelot-py 1.0.9 does not provide the extra 'cv'\u001B[0m\u001B[33m\r\n",
      "\u001B[0mCollecting click>=8.0.1 (from camelot-py[cv])\r\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\r\n",
      "Collecting chardet>=5.1.0 (from camelot-py[cv])\r\n",
      "  Downloading chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\r\n",
      "Requirement already satisfied: numpy>=1.24.4 in /Users/muradeliyev/Library/Python/3.9/lib/python/site-packages (from camelot-py[cv]) (1.26.4)\r\n",
      "Requirement already satisfied: openpyxl>=3.1.0 in /Users/muradeliyev/Library/Python/3.9/lib/python/site-packages (from camelot-py[cv]) (3.1.4)\r\n",
      "Requirement already satisfied: pdfminer-six>=20240706 in /Users/muradeliyev/Library/Python/3.9/lib/python/site-packages (from camelot-py[cv]) (20251107)\r\n",
      "Collecting pypdf<4.0,>=3.17 (from camelot-py[cv])\r\n",
      "  Downloading pypdf-3.17.4-py3-none-any.whl.metadata (7.5 kB)\r\n",
      "Requirement already satisfied: pandas>=1.5.3 in /Users/muradeliyev/Library/Python/3.9/lib/python/site-packages (from camelot-py[cv]) (2.2.2)\r\n",
      "Collecting tabulate>=0.9.0 (from camelot-py[cv])\r\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\r\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /Users/muradeliyev/Library/Python/3.9/lib/python/site-packages (from camelot-py[cv]) (4.15.0)\r\n",
      "Collecting opencv-python-headless>=4.7.0.68 (from camelot-py[cv])\r\n",
      "  Downloading opencv_python_headless-4.13.0.90-cp37-abi3-macosx_13_0_arm64.whl.metadata (19 kB)\r\n",
      "Requirement already satisfied: pypdfium2>=4 in /Users/muradeliyev/Library/Python/3.9/lib/python/site-packages (from camelot-py[cv]) (5.3.0)\r\n",
      "Requirement already satisfied: pillow>=10.4.0 in /Users/muradeliyev/Library/Python/3.9/lib/python/site-packages (from camelot-py[cv]) (10.4.0)\r\n",
      "Collecting numpy>=1.24.4 (from camelot-py[cv])\r\n",
      "  Downloading numpy-2.0.2-cp39-cp39-macosx_14_0_arm64.whl.metadata (60 kB)\r\n",
      "Requirement already satisfied: et-xmlfile in /Users/muradeliyev/Library/Python/3.9/lib/python/site-packages (from openpyxl>=3.1.0->camelot-py[cv]) (1.1.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/muradeliyev/Library/Python/3.9/lib/python/site-packages (from pandas>=1.5.3->camelot-py[cv]) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/muradeliyev/Library/Python/3.9/lib/python/site-packages (from pandas>=1.5.3->camelot-py[cv]) (2024.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/muradeliyev/Library/Python/3.9/lib/python/site-packages (from pandas>=1.5.3->camelot-py[cv]) (2024.1)\r\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in /Users/muradeliyev/Library/Python/3.9/lib/python/site-packages (from pdfminer-six>=20240706->camelot-py[cv]) (3.3.2)\r\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /Users/muradeliyev/Library/Python/3.9/lib/python/site-packages (from pdfminer-six>=20240706->camelot-py[cv]) (46.0.3)\r\n",
      "Requirement already satisfied: cffi>=2.0.0 in /Users/muradeliyev/Library/Python/3.9/lib/python/site-packages (from cryptography>=36.0.0->pdfminer-six>=20240706->camelot-py[cv]) (2.0.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas>=1.5.3->camelot-py[cv]) (1.15.0)\r\n",
      "Requirement already satisfied: pycparser in /Users/muradeliyev/Library/Python/3.9/lib/python/site-packages (from cffi>=2.0.0->cryptography>=36.0.0->pdfminer-six>=20240706->camelot-py[cv]) (2.23)\r\n",
      "Downloading chardet-5.2.0-py3-none-any.whl (199 kB)\r\n",
      "Downloading click-8.1.8-py3-none-any.whl (98 kB)\r\n",
      "Downloading opencv_python_headless-4.13.0.90-cp37-abi3-macosx_13_0_arm64.whl (46.0 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m46.0/46.0 MB\u001B[0m \u001B[31m11.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading numpy-2.0.2-cp39-cp39-macosx_14_0_arm64.whl (5.3 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m5.3/5.3 MB\u001B[0m \u001B[31m10.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading pypdf-3.17.4-py3-none-any.whl (278 kB)\r\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\r\n",
      "Downloading camelot_py-1.0.9-py3-none-any.whl (66 kB)\r\n",
      "Installing collected packages: tabulate, pypdf, numpy, click, chardet, opencv-python-headless, camelot-py\r\n",
      "\u001B[33m  WARNING: The script tabulate is installed in '/Users/muradeliyev/Library/Python/3.9/bin' which is not on PATH.\r\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\r\n",
      "  NOTE: The current PATH contains path(s) starting with `~`, which may not be expanded by all applications.\u001B[0m\u001B[33m\r\n",
      "\u001B[0m  Attempting uninstall: numpy\r\n",
      "    Found existing installation: numpy 1.26.4\r\n",
      "    Uninstalling numpy-1.26.4:\r\n",
      "      Successfully uninstalled numpy-1.26.4\r\n",
      "\u001B[33m  WARNING: The scripts f2py and numpy-config are installed in '/Users/muradeliyev/Library/Python/3.9/bin' which is not on PATH.\r\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\r\n",
      "  NOTE: The current PATH contains path(s) starting with `~`, which may not be expanded by all applications.\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\u001B[33m  WARNING: The script chardetect is installed in '/Users/muradeliyev/Library/Python/3.9/bin' which is not on PATH.\r\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\r\n",
      "  NOTE: The current PATH contains path(s) starting with `~`, which may not be expanded by all applications.\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\u001B[33m  WARNING: The script camelot is installed in '/Users/muradeliyev/Library/Python/3.9/bin' which is not on PATH.\r\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\r\n",
      "  NOTE: The current PATH contains path(s) starting with `~`, which may not be expanded by all applications.\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "ultralytics 8.3.36 requires numpy<2.0.0; sys_platform == \"darwin\", but you have numpy 2.0.2 which is incompatible.\u001B[0m\u001B[31m\r\n",
      "\u001B[0mSuccessfully installed camelot-py-1.0.9 chardet-5.2.0 click-8.1.8 numpy-2.0.2 opencv-python-headless-4.13.0.90 pypdf-3.17.4 tabulate-0.9.0\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.3\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001B[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1d491a409d320b85"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T20:24:15.659431Z",
     "start_time": "2026-01-19T20:24:13.982934Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import camelot\n",
    "\n",
    "tables = camelot.read_pdf('../data/PDF_version_1000/15_9_19_A_1997_07_30.pdf', pages='all', flavor='lattice')\n",
    "\n",
    "for i, table in enumerate(tables):\n",
    "    table.to_csv(f'table_{i}.csv')"
   ],
   "id": "9df32b0ea39a04f7",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T20:21:50.513617Z",
     "start_time": "2026-01-19T20:21:50.022525Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pdfplumber\n",
    "\n",
    "with pdfplumber.open('../data/PDF_version_1000/15_9_19_A_1997_07_30.pdf') as pdf:\n",
    "    text = pdf.pages[0].extract_text()\n",
    "\n",
    "    start = text.find('Summary of activities')\n",
    "    end = text.find('Operations')\n",
    "\n",
    "    print(text[start:end])"
   ],
   "id": "fd1e8122b57fbb1a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of activities (24 Hours)\n",
      "FINISHED HANDLING MILLING BHA. MU & TIH WITH DRILLING BHA TO TD.\n",
      "DRILLED FROM 2202 - 2405 M. SLIDING AS DIRECTIONAL PROGRAM REQUIRED\n",
      "TO DROP ANGLE & TURN HOLE TOWARD OBJECTIVE.\n",
      "Summary of planned activities (24 Hours)\n",
      "CONTINUE DRILLING 8 1/2\" HOLE.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T20:53:45.074877Z",
     "start_time": "2026-01-19T20:53:44.538106Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pdfplumber\n",
    "import cv2\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def extract_sections_with_cv(pdf_file):\n",
    "    \"\"\"Extract sections using computer vision with quality filters\"\"\"\n",
    "\n",
    "    sections = []\n",
    "\n",
    "    with pdfplumber.open(pdf_file) as pdf:\n",
    "        for page_num, page in enumerate(pdf.pages, 1):\n",
    "\n",
    "            # Convert page to image\n",
    "            img = page.to_image(resolution=150)\n",
    "            pil_img = img.original\n",
    "\n",
    "            # Convert to OpenCV format\n",
    "            opencv_img = cv2.cvtColor(np.array(pil_img), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            # Convert to HSV for better color detection\n",
    "            hsv = cv2.cvtColor(opencv_img, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "            # Define range for gray color (section headers)\n",
    "            lower_gray = np.array([0, 0, 100])\n",
    "            upper_gray = np.array([180, 50, 220])\n",
    "\n",
    "            # Create mask for gray regions\n",
    "            mask = cv2.inRange(hsv, lower_gray, upper_gray)\n",
    "\n",
    "            # Find contours (gray boxes)\n",
    "            contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "            # Filter contours by size\n",
    "            for contour in contours:\n",
    "                x, y, w, h = cv2.boundingRect(contour)\n",
    "\n",
    "                # Only keep large horizontal boxes (section headers)\n",
    "                # Section headers are typically: wide, not too tall\n",
    "                if w > 200 and 15 < h < 40:\n",
    "\n",
    "                    # Convert to PDF coordinates\n",
    "                    scale = page.width / opencv_img.shape[1]\n",
    "\n",
    "                    pdf_x0 = x * scale\n",
    "                    pdf_y0 = y * scale\n",
    "                    pdf_x1 = (x + w) * scale\n",
    "                    pdf_y1 = (y + h) * scale\n",
    "\n",
    "                    # Extract text\n",
    "                    try:\n",
    "                        cropped = page.crop((pdf_x0, pdf_y0, pdf_x1, pdf_y1))\n",
    "                        text = cropped.extract_text()\n",
    "\n",
    "                        if text and len(text.strip()) > 3:\n",
    "                            text = text.strip()\n",
    "\n",
    "                            # QUALITY FILTERS\n",
    "                            # 1. Check for duplicate characters (TTiimmee)\n",
    "                            if has_duplicate_chars(text):\n",
    "                                continue\n",
    "\n",
    "                            # 2. Must not be too long (real headers are concise)\n",
    "                            if len(text) > 100:\n",
    "                                continue\n",
    "\n",
    "                            # 3. Should not have too many numbers/special chars\n",
    "                            if count_special_chars(text) > len(text) * 0.3:\n",
    "                                continue\n",
    "\n",
    "                            sections.append({\n",
    "                                'page': page_num,\n",
    "                                'text': text,\n",
    "                                'y_position': pdf_y0,\n",
    "                                'height': h\n",
    "                            })\n",
    "                    except:\n",
    "                        continue\n",
    "\n",
    "    # Remove duplicates (keep first occurrence)\n",
    "    seen = set()\n",
    "    unique_sections = []\n",
    "    for s in sections:\n",
    "        if s['text'] not in seen:\n",
    "            seen.add(s['text'])\n",
    "            unique_sections.append(s)\n",
    "\n",
    "    # Sort by page first, then by y_position (top to bottom)\n",
    "    unique_sections.sort(key=lambda x: (x['page'], x['y_position']))\n",
    "\n",
    "    return unique_sections\n",
    "\n",
    "def has_duplicate_chars(text):\n",
    "    \"\"\"Check if text has duplicate consecutive characters (OCR error)\"\"\"\n",
    "    duplicate_count = 0\n",
    "    for i in range(len(text) - 1):\n",
    "        if text[i] == text[i + 1] and text[i].isalpha():\n",
    "            duplicate_count += 1\n",
    "\n",
    "    return duplicate_count > len(text) * 0.2\n",
    "\n",
    "def count_special_chars(text):\n",
    "    \"\"\"Count special characters and numbers\"\"\"\n",
    "    return sum(1 for c in text if not c.isalpha() and not c.isspace())\n",
    "\n",
    "# Test\n",
    "pdf_file = '../data/PDF_version_1000/15_9_19_A_1997_07_30.pdf'\n",
    "sections = extract_sections_with_cv(pdf_file)\n",
    "\n",
    "print(f\"Found {len(sections)} valid sections:\\n\")\n",
    "for s in sections:\n",
    "    print(f\"Page {s['page']}, Y={s['y_position']:.1f}: {s['text']}\")"
   ],
   "id": "9a4fa28b0828e95e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 valid sections:\n",
      "\n",
      "Page 1, Y=244.2: Summary of activities (24 Hours)\n",
      "Page 1, Y=296.1: Summary of planned activities (24 Hours)\n",
      "Page 1, Y=333.0: Operations\n",
      "Page 1, Y=506.2: Drilling Fluid\n",
      "Page 2, Y=83.0: Pore Pressure\n",
      "Page 2, Y=141.1: Survey Station\n",
      "Page 2, Y=224.1: Lithology Information\n",
      "Page 2, Y=282.6: Gas Reading Information\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T20:45:54.714370Z",
     "start_time": "2026-01-19T20:45:54.693540Z"
    }
   },
   "cell_type": "code",
   "source": "path = \"../data/PDF_version_1000/15_9_19_A_1997_07_30.pdf\"",
   "id": "dd3a8188084d5051",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T20:46:04.282716Z",
     "start_time": "2026-01-19T20:46:02.587654Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import camelot\n",
    "\n",
    "tables = camelot.read_pdf(path, pages='all', flavor='lattice')\n",
    "\n",
    "for i, table in enumerate(tables):\n",
    "    table.to_csv(f'table_{i}.csv')"
   ],
   "id": "6127c0050d2961e7",
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T20:59:26.390969Z",
     "start_time": "2026-01-19T20:59:18.209098Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pdfplumber\n",
    "import cv2\n",
    "import numpy as np\n",
    "import camelot\n",
    "import re\n",
    "\n",
    "def extract_sections_with_cv(pdf_file):\n",
    "    \"\"\"Extract sections with their positions\"\"\"\n",
    "\n",
    "    sections = []\n",
    "\n",
    "    with pdfplumber.open(pdf_file) as pdf:\n",
    "        for page_num, page in enumerate(pdf.pages, 1):\n",
    "\n",
    "            img = page.to_image(resolution=150)\n",
    "            pil_img = img.original\n",
    "            opencv_img = cv2.cvtColor(np.array(pil_img), cv2.COLOR_RGB2BGR)\n",
    "            hsv = cv2.cvtColor(opencv_img, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "            lower_gray = np.array([0, 0, 100])\n",
    "            upper_gray = np.array([180, 50, 220])\n",
    "            mask = cv2.inRange(hsv, lower_gray, upper_gray)\n",
    "            contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "            for contour in contours:\n",
    "                x, y, w, h = cv2.boundingRect(contour)\n",
    "\n",
    "                if w > 200 and 15 < h < 40:\n",
    "                    scale = page.width / opencv_img.shape[1]\n",
    "                    pdf_y = y * scale\n",
    "\n",
    "                    try:\n",
    "                        cropped = page.crop((x * scale, pdf_y, (x + w) * scale, (y + h) * scale))\n",
    "                        text = cropped.extract_text()\n",
    "\n",
    "                        if text and len(text.strip()) > 3:\n",
    "                            text = text.strip()\n",
    "                            if has_duplicate_chars(text) or len(text) > 100:\n",
    "                                continue\n",
    "\n",
    "                            sections.append({\n",
    "                                'page': page_num,\n",
    "                                'text': text,\n",
    "                                'y_position': pdf_y,\n",
    "                                'page_height': page.height\n",
    "                            })\n",
    "                    except:\n",
    "                        continue\n",
    "\n",
    "    # Remove duplicates and sort\n",
    "    seen = set()\n",
    "    unique_sections = []\n",
    "    for s in sections:\n",
    "        if s['text'] not in seen:\n",
    "            seen.add(s['text'])\n",
    "            unique_sections.append(s)\n",
    "\n",
    "    unique_sections.sort(key=lambda x: (x['page'], x['y_position']))\n",
    "    return unique_sections\n",
    "\n",
    "def has_duplicate_chars(text):\n",
    "    duplicate_count = 0\n",
    "    for i in range(len(text) - 1):\n",
    "        if text[i] == text[i + 1] and text[i].isalpha():\n",
    "            duplicate_count += 1\n",
    "    return duplicate_count > len(text) * 0.2\n",
    "\n",
    "def extract_tables_by_sections(pdf_file):\n",
    "    \"\"\"Extract tables using section regions\"\"\"\n",
    "\n",
    "    # Get sections\n",
    "    sections = extract_sections_with_cv(pdf_file)\n",
    "\n",
    "    print(f\"Found {len(sections)} sections\\n\")\n",
    "\n",
    "    # Get page info\n",
    "    with pdfplumber.open(pdf_file) as pdf:\n",
    "        page_width = pdf.pages[0].width\n",
    "        page_height = pdf.pages[0].height\n",
    "\n",
    "    # Extract common tables (from top of page 1 to first section)\n",
    "    if sections:\n",
    "        first_section = sections[0]\n",
    "        if first_section['page'] == 1:\n",
    "            print(\"=\" * 70)\n",
    "            print(\"EXTRACTING COMMON TABLES (before first section)\")\n",
    "            print(\"=\" * 70)\n",
    "\n",
    "            # Region from top to first section\n",
    "            y1 = page_height  # Top of page\n",
    "            y2 = page_height - first_section['y_position'] + 10  # First section\n",
    "            x1 = 0\n",
    "            x2 = page_width\n",
    "\n",
    "            region = f'{x1},{y1},{x2},{y2}'\n",
    "            print(f\"Page 1: Common Report Info\")\n",
    "            print(f\"  Region: {region}\")\n",
    "\n",
    "            try:\n",
    "                tables = camelot.read_pdf(\n",
    "                    pdf_file,\n",
    "                    pages='1',\n",
    "                    flavor='lattice',\n",
    "                    table_regions=[region]\n",
    "                )\n",
    "\n",
    "                # Save all common tables\n",
    "                for idx, table in enumerate(tables):\n",
    "                    filename = f'common_{idx + 1}.csv'\n",
    "                    table.to_csv(filename)\n",
    "                    print(f\"  ✓ Saved: {filename} ({table.df.shape})\")\n",
    "\n",
    "                if len(tables) == 0:\n",
    "                    print(f\"  ✗ No table found\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"  ✗ Error: {e}\")\n",
    "\n",
    "            print()\n",
    "\n",
    "    # Extract tables for each section\n",
    "    print(\"=\" * 70)\n",
    "    print(\"EXTRACTING SECTION TABLES\")\n",
    "    print(\"=\" * 70)\n",
    "    print()\n",
    "\n",
    "    for i, section in enumerate(sections):\n",
    "        page_num = section['page']\n",
    "        page_height = section['page_height']\n",
    "\n",
    "        # Section Y position (from top)\n",
    "        section_y_top = section['y_position']\n",
    "\n",
    "        # Find next section on same page or use page bottom\n",
    "        if i + 1 < len(sections) and sections[i + 1]['page'] == page_num:\n",
    "            next_y = sections[i + 1]['y_position']\n",
    "        else:\n",
    "            next_y = page_height\n",
    "\n",
    "        # Convert to PDF coordinates (from bottom-left)\n",
    "        y1 = page_height - section_y_top - 20  # Start below section header\n",
    "        y2 = page_height - next_y + 10  # End at next section\n",
    "\n",
    "        # Full width\n",
    "        x1 = 0\n",
    "        x2 = page_width\n",
    "\n",
    "        # Create region string\n",
    "        region = f'{x1},{y1},{x2},{y2}'\n",
    "\n",
    "        print(f\"Page {page_num}: {section['text']}\")\n",
    "        print(f\"  Region: {region}\")\n",
    "\n",
    "        # Extract table in this region\n",
    "        try:\n",
    "            tables = camelot.read_pdf(\n",
    "                pdf_file,\n",
    "                pages=str(page_num),\n",
    "                flavor='lattice',\n",
    "                table_regions=[region]\n",
    "            )\n",
    "\n",
    "            if len(tables) > 0:\n",
    "                # Normalize section name for filename\n",
    "                name = re.sub(r'\\([^)]*\\)', '', section['text'])\n",
    "                name = name.lower().strip()\n",
    "                name = re.sub(r'[^\\w\\s-]', '', name)\n",
    "                name = re.sub(r'[-\\s]+', '_', name)\n",
    "\n",
    "                filename = f'{name}.csv'\n",
    "                tables[0].to_csv(filename)\n",
    "                print(f\"  ✓ Saved: {filename} ({tables[0].df.shape})\")\n",
    "            else:\n",
    "                print(f\"  ✗ No table found\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error: {e}\")\n",
    "\n",
    "        print()\n",
    "\n",
    "# Run\n",
    "pdf_file = '../data/PDF_version_1000/15_9_19_A_1997_07_30.pdf'\n",
    "extract_tables_by_sections(pdf_file)"
   ],
   "id": "ee5032852dbfad53",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 sections\n",
      "\n",
      "======================================================================\n",
      "EXTRACTING COMMON TABLES (before first section)\n",
      "======================================================================\n",
      "Page 1: Common Report Info\n",
      "  Region: 0,842,595,607.7620967741935\n",
      "  ✓ Saved: common_1.csv ((7, 3))\n",
      "  ✓ Saved: common_2.csv ((10, 3))\n",
      "  ✓ Saved: common_3.csv ((17, 3))\n",
      "\n",
      "======================================================================\n",
      "EXTRACTING SECTION TABLES\n",
      "======================================================================\n",
      "\n",
      "Page 1: Summary of activities (24 Hours)\n",
      "  Region: 0,577.7620967741935,595,555.9395161290322\n",
      "  ✗ No table found\n",
      "\n",
      "Page 1: Summary of planned activities (24 Hours)\n",
      "  Region: 0,525.9395161290322,595,518.991935483871\n",
      "  ✗ No table found\n",
      "\n",
      "Page 1: Operations\n",
      "  Region: 0,488.991935483871,595,345.77016129032256\n",
      "  ✓ Saved: operations.csv ((13, 6))\n",
      "\n",
      "Page 1: Drilling Fluid\n",
      "  Region: 0,315.77016129032256,595,10\n",
      "  ✓ Saved: drilling_fluid.csv ((29, 3))\n",
      "\n",
      "Page 2: Pore Pressure\n",
      "  Region: 0,738.9879032258065,595,710.9274193548387\n",
      "  ✗ No table found\n",
      "\n",
      "Page 2: Survey Station\n",
      "  Region: 0,680.9274193548387,595,627.9153225806451\n",
      "  ✗ No table found\n",
      "\n",
      "Page 2: Lithology Information\n",
      "  Region: 0,597.9153225806451,595,569.375\n",
      "  ✗ No table found\n",
      "\n",
      "Page 2: Gas Reading Information\n",
      "  Region: 0,539.375,595,10\n",
      "  ✓ Saved: gas_reading_information.csv ((4, 13))\n",
      "\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T21:10:29.980547Z",
     "start_time": "2026-01-19T21:10:29.923947Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def csv_to_json(csv_file=\"common_3.csv\"):\n",
    "    \"\"\"Convert CSV to JSON - auto-detect structure\"\"\"\n",
    "\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    # Check if columns are unnamed (Unnamed: 0, Unnamed: 1, etc.)\n",
    "    has_unnamed = any('unnamed' in str(col).lower() for col in df.columns)\n",
    "\n",
    "    if has_unnamed:\n",
    "        # Key-value structure\n",
    "        result = {}\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            key = row.iloc[0]\n",
    "            value = None\n",
    "\n",
    "            # Get first non-null value\n",
    "            for i in range(1, len(row)):\n",
    "                if pd.notna(row.iloc[i]):\n",
    "                    value = row.iloc[i]\n",
    "                    break\n",
    "\n",
    "            if isinstance(key, str):\n",
    "                key = key.rstrip(':').strip()\n",
    "                key = key.lower().replace(' ', '_').replace('(', '').replace(')', '').replace('/', '_')\n",
    "                result[key] = value\n",
    "\n",
    "        return result\n",
    "\n",
    "    else:\n",
    "        # Tabular structure - keep as is\n",
    "        # Replace NaN with None for proper JSON\n",
    "        df = df.replace({np.nan: None})\n",
    "        return df.to_dict(orient='records')\n",
    "\n",
    "# Test with key-value CSV\n",
    "print(\"Key-Value (Unnamed columns):\")\n",
    "data1 = csv_to_json(\"common_3.csv\")\n",
    "print(json.dumps(data1, indent=2))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "# Test with tabular CSV\n",
    "print(\"Tabular (Named columns):\")\n",
    "data2 = csv_to_json(\"operations.csv\")\n",
    "print(json.dumps(data2[:2], indent=2))  # Show first 2 rows"
   ],
   "id": "98bbab8fdb4096f3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key-Value (Unnamed columns):\n",
      "{\n",
      "  \"status\": \"normal\",\n",
      "  \"report_creation_time\": \"2018-05-03 13:53\",\n",
      "  \"report_number\": \"6\",\n",
      "  \"days_ahead_behind_+_-\": null,\n",
      "  \"operator\": \"Statoil\",\n",
      "  \"rig_name\": \"BYFORD DOLPHIN\",\n",
      "  \"drilling_contractor\": null,\n",
      "  \"spud_date\": \"1997-07-25 00:00\",\n",
      "  \"wellbore_type\": null,\n",
      "  \"elevation_rkb-msl_m\": \"25\",\n",
      "  \"water_depth_msl_m\": \"84\",\n",
      "  \"tight_well\": \"Y\",\n",
      "  \"hpht\": \"Y\",\n",
      "  \"temperature_\": null,\n",
      "  \"pressure_\": null,\n",
      "  \"date_well_complete\": \"1997-08-30\"\n",
      "}\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Tabular (Named columns):\n",
      "[\n",
      "  {\n",
      "    \"Start\\ntime\": \"00:00\",\n",
      "    \"End\\ntime\": \"01:00\",\n",
      "    \"End Depth\\nmMD\": 2202,\n",
      "    \"Main - Sub Activity\": \"interruption -- sidetrack\",\n",
      "    \"State\": \"ok\",\n",
      "    \"Remark\": \"FINISHED DRILLING TO 2202 M - TOTAL OF 19 M OF FORMATION DRILLED BEYOND WINDOW'S BOTTOM. PU & SO THROUGH WINDOW 5 TIMES - N\\nO PROBLEMS.\"\n",
      "  },\n",
      "  {\n",
      "    \"Start\\ntime\": \"01:00\",\n",
      "    \"End\\ntime\": \"02:30\",\n",
      "    \"End Depth\\nmMD\": 2175,\n",
      "    \"Main - Sub Activity\": \"drilling -- circulating cond\\nitioning\",\n",
      "    \"State\": \"ok\",\n",
      "    \"Remark\": \"PU ABOVE TOP OF WINDOW & PUMPED 5 M3 HI-VIS PILL. CIRCULATED HOLE CLEAN AT 2000 LPM AT 195 BAR. FLUSHED CHOKE & KILL LINES & B\\nOOSTED RISER.\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "execution_count": 75
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
